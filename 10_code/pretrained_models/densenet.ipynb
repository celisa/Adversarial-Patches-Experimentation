{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/codespace/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/codespace/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load DenseNet model\n",
    "densenet_model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n",
    "densenet_model = densenet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# preapre data\n",
    "\n",
    "# PARAMS\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target = 1 #automobile\n",
    "\n",
    "#CIFAR-10 image tensor mean and std\n",
    "NORM_MEAN = [0.4914, 0.4822, 0.4465]\n",
    "NORM_STD = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "\n",
    "# TODO: should we apply the below transformations?\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN,\n",
    "                         std=NORM_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN,\n",
    "                         std=NORM_STD)\n",
    "])\n",
    "\n",
    "transform_image = transforms.ToPILImage()\n",
    "\n",
    "print('==> Preparing data..')\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../00_data/', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../../00_data/', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "\n",
    "def train(net, epochs, batch_size, lr, reg, log_every_n=50, trainloader=None, testloader=None):\n",
    "    print('==> Preparing data..')\n",
    "\n",
    "    best_acc = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=reg)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs*0.5), int(epochs*0.75)], gamma=0.1)\n",
    "\n",
    "    global_steps = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            global_steps += 1\n",
    "\n",
    "            if global_steps % log_every_n == 0:\n",
    "                end = time.time()\n",
    "                num_examples_per_second = log_every_n * batch_size / (end - start)\n",
    "                print(\"[Step=%d]\\tLoss=%.4f\\tacc=%.4f\\t%.1f examples/second\"\n",
    "                      % (global_steps, train_loss / (batch_idx + 1), (correct / total), num_examples_per_second))\n",
    "                start = time.time()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        num_val_steps = len(testloader)\n",
    "        val_acc = correct / total\n",
    "        print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"pretrained_densenet_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=4.1591\tacc=0.2595\t2771.6 examples/second\n",
      "[Step=100]\tLoss=2.7506\tacc=0.4068\t3207.6 examples/second\n",
      "[Step=150]\tLoss=2.1912\tacc=0.4809\t3394.1 examples/second\n",
      "Test Loss=0.8868, Test acc=0.6949\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.8313\tacc=0.7061\t1695.0 examples/second\n",
      "[Step=250]\tLoss=0.7931\tacc=0.7261\t3199.4 examples/second\n",
      "[Step=300]\tLoss=0.7619\tacc=0.7375\t3193.0 examples/second\n",
      "[Step=350]\tLoss=0.7470\tacc=0.7434\t3161.9 examples/second\n",
      "Test Loss=0.7088, Test acc=0.7572\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.5839\tacc=0.8008\t1728.1 examples/second\n",
      "[Step=450]\tLoss=0.5551\tacc=0.8079\t3182.0 examples/second\n",
      "[Step=500]\tLoss=0.5604\tacc=0.8064\t3185.2 examples/second\n",
      "[Step=550]\tLoss=0.5558\tacc=0.8074\t3194.7 examples/second\n",
      "Test Loss=0.6504, Test acc=0.7791\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.4608\tacc=0.8428\t1751.6 examples/second\n",
      "[Step=650]\tLoss=0.4405\tacc=0.8491\t3208.5 examples/second\n",
      "[Step=700]\tLoss=0.4331\tacc=0.8500\t3211.6 examples/second\n",
      "[Step=750]\tLoss=0.4329\tacc=0.8512\t3173.1 examples/second\n",
      "Test Loss=0.6252, Test acc=0.7938\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.3332\tacc=0.8901\t1741.0 examples/second\n",
      "[Step=850]\tLoss=0.3345\tacc=0.8912\t3197.3 examples/second\n",
      "[Step=900]\tLoss=0.3393\tacc=0.8874\t3167.6 examples/second\n",
      "[Step=950]\tLoss=0.3398\tacc=0.8858\t3167.5 examples/second\n",
      "Test Loss=0.6385, Test acc=0.7960\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.2401\tacc=0.9246\t1741.5 examples/second\n",
      "[Step=1050]\tLoss=0.2438\tacc=0.9224\t3186.5 examples/second\n",
      "[Step=1100]\tLoss=0.2471\tacc=0.9189\t3027.2 examples/second\n",
      "[Step=1150]\tLoss=0.2462\tacc=0.9192\t3211.4 examples/second\n",
      "Test Loss=0.6558, Test acc=0.7997\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.1751\tacc=0.9502\t1750.2 examples/second\n",
      "[Step=1250]\tLoss=0.1734\tacc=0.9480\t3163.0 examples/second\n",
      "[Step=1300]\tLoss=0.1741\tacc=0.9458\t3213.5 examples/second\n",
      "[Step=1350]\tLoss=0.1753\tacc=0.9447\t3168.4 examples/second\n",
      "Test Loss=0.7111, Test acc=0.7966\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.1121\tacc=0.9697\t1777.1 examples/second\n",
      "[Step=1450]\tLoss=0.1144\tacc=0.9692\t3219.5 examples/second\n",
      "[Step=1500]\tLoss=0.1185\tacc=0.9674\t3205.8 examples/second\n",
      "[Step=1550]\tLoss=0.1206\tacc=0.9660\t3178.6 examples/second\n",
      "Test Loss=0.7502, Test acc=0.7951\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0726\tacc=0.9835\t1782.4 examples/second\n",
      "[Step=1650]\tLoss=0.0764\tacc=0.9818\t3190.3 examples/second\n",
      "[Step=1700]\tLoss=0.0764\tacc=0.9814\t3192.8 examples/second\n",
      "[Step=1750]\tLoss=0.0798\tacc=0.9791\t3179.4 examples/second\n",
      "Test Loss=0.7858, Test acc=0.8005\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0479\tacc=0.9909\t1744.6 examples/second\n",
      "[Step=1850]\tLoss=0.0506\tacc=0.9892\t3180.2 examples/second\n",
      "[Step=1900]\tLoss=0.0529\tacc=0.9882\t3185.3 examples/second\n",
      "[Step=1950]\tLoss=0.0540\tacc=0.9879\t3204.4 examples/second\n",
      "Test Loss=0.8101, Test acc=0.8039\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0317\tacc=0.9948\t1744.9 examples/second\n",
      "[Step=2050]\tLoss=0.0331\tacc=0.9937\t3153.6 examples/second\n",
      "[Step=2100]\tLoss=0.0336\tacc=0.9938\t3156.1 examples/second\n",
      "[Step=2150]\tLoss=0.0339\tacc=0.9938\t3211.9 examples/second\n",
      "Test Loss=0.8072, Test acc=0.8055\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0294\tacc=0.9957\t1709.6 examples/second\n",
      "[Step=2250]\tLoss=0.0288\tacc=0.9962\t3159.3 examples/second\n",
      "[Step=2300]\tLoss=0.0290\tacc=0.9960\t3170.3 examples/second\n",
      "[Step=2350]\tLoss=0.0289\tacc=0.9960\t3169.4 examples/second\n",
      "Test Loss=0.8199, Test acc=0.8031\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0252\tacc=0.9972\t1790.2 examples/second\n",
      "[Step=2450]\tLoss=0.0254\tacc=0.9972\t3154.3 examples/second\n",
      "[Step=2500]\tLoss=0.0260\tacc=0.9966\t3168.2 examples/second\n",
      "Test Loss=0.8230, Test acc=0.8014\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0345\tacc=0.9941\t1782.9 examples/second\n",
      "[Step=2600]\tLoss=0.0275\tacc=0.9957\t3209.4 examples/second\n",
      "[Step=2650]\tLoss=0.0252\tacc=0.9966\t3156.5 examples/second\n",
      "[Step=2700]\tLoss=0.0249\tacc=0.9966\t3170.7 examples/second\n",
      "Test Loss=0.8255, Test acc=0.8056\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0267\tacc=0.9954\t1744.4 examples/second\n",
      "[Step=2800]\tLoss=0.0256\tacc=0.9964\t3168.1 examples/second\n",
      "[Step=2850]\tLoss=0.0248\tacc=0.9970\t3148.3 examples/second\n",
      "[Step=2900]\tLoss=0.0247\tacc=0.9969\t3151.5 examples/second\n",
      "Test Loss=0.8313, Test acc=0.8032\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0195\tacc=0.9984\t1777.8 examples/second\n",
      "[Step=3000]\tLoss=0.0207\tacc=0.9982\t3197.9 examples/second\n",
      "[Step=3050]\tLoss=0.0221\tacc=0.9978\t3160.4 examples/second\n",
      "[Step=3100]\tLoss=0.0220\tacc=0.9979\t3138.1 examples/second\n",
      "Test Loss=0.8351, Test acc=0.8047\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0215\tacc=0.9975\t1781.4 examples/second\n",
      "[Step=3200]\tLoss=0.0227\tacc=0.9968\t3161.8 examples/second\n",
      "[Step=3250]\tLoss=0.0231\tacc=0.9971\t3049.9 examples/second\n",
      "[Step=3300]\tLoss=0.0231\tacc=0.9969\t3108.7 examples/second\n",
      "Test Loss=0.8317, Test acc=0.8045\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0215\tacc=0.9980\t1791.3 examples/second\n",
      "[Step=3400]\tLoss=0.0230\tacc=0.9973\t3166.8 examples/second\n",
      "[Step=3450]\tLoss=0.0219\tacc=0.9977\t3152.1 examples/second\n",
      "[Step=3500]\tLoss=0.0222\tacc=0.9977\t3119.0 examples/second\n",
      "Test Loss=0.8307, Test acc=0.8049\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0210\tacc=0.9980\t1758.3 examples/second\n",
      "[Step=3600]\tLoss=0.0229\tacc=0.9976\t3171.1 examples/second\n",
      "[Step=3650]\tLoss=0.0215\tacc=0.9980\t3141.1 examples/second\n",
      "[Step=3700]\tLoss=0.0216\tacc=0.9979\t3159.3 examples/second\n",
      "Test Loss=0.8336, Test acc=0.8035\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0215\tacc=0.9974\t1777.2 examples/second\n",
      "[Step=3800]\tLoss=0.0221\tacc=0.9974\t3168.8 examples/second\n",
      "[Step=3850]\tLoss=0.0215\tacc=0.9977\t3151.4 examples/second\n",
      "[Step=3900]\tLoss=0.0216\tacc=0.9977\t3101.3 examples/second\n",
      "Test Loss=0.8317, Test acc=0.8039\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0005\n",
    "\n",
    "train(net=densenet_model, epochs=epochs, batch_size=batch_size, lr=learning_rate, reg=weight_decay,\n",
    "      trainloader=trainloader, testloader=testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
